{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rahul Kumar Goyal (rxg170030)\n",
    "# Varun Kumar Manohara Selvan (vxm170030)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the required libraries and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "import warnings; warnings.simplefilter('ignore')\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.utils import np_utils\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, PolynomialFeatures, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, validation_curve, GridSearchCV, KFold, cross_val_score\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor, BaggingRegressor, RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.tree import DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('ENB2012_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.columns = [\"relative_compactness\", \"surface_area\", \"wall_area\",\n",
    "              \"roof_area\", \"overall_height\", \"orientation\", \"glazing_area\",\n",
    "             \"glazing_area_distribution\", \"heating_load\", \"cooling_load\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>relative_compactness</th>\n",
       "      <th>surface_area</th>\n",
       "      <th>wall_area</th>\n",
       "      <th>roof_area</th>\n",
       "      <th>overall_height</th>\n",
       "      <th>orientation</th>\n",
       "      <th>glazing_area</th>\n",
       "      <th>glazing_area_distribution</th>\n",
       "      <th>heating_load</th>\n",
       "      <th>cooling_load</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.98</td>\n",
       "      <td>514.5</td>\n",
       "      <td>294.0</td>\n",
       "      <td>110.25</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>15.55</td>\n",
       "      <td>21.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.98</td>\n",
       "      <td>514.5</td>\n",
       "      <td>294.0</td>\n",
       "      <td>110.25</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>15.55</td>\n",
       "      <td>21.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.98</td>\n",
       "      <td>514.5</td>\n",
       "      <td>294.0</td>\n",
       "      <td>110.25</td>\n",
       "      <td>7.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>15.55</td>\n",
       "      <td>21.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.98</td>\n",
       "      <td>514.5</td>\n",
       "      <td>294.0</td>\n",
       "      <td>110.25</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>15.55</td>\n",
       "      <td>21.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.90</td>\n",
       "      <td>563.5</td>\n",
       "      <td>318.5</td>\n",
       "      <td>122.50</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>20.84</td>\n",
       "      <td>28.28</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   relative_compactness  surface_area  wall_area  roof_area  overall_height  \\\n",
       "0                  0.98         514.5      294.0     110.25             7.0   \n",
       "1                  0.98         514.5      294.0     110.25             7.0   \n",
       "2                  0.98         514.5      294.0     110.25             7.0   \n",
       "3                  0.98         514.5      294.0     110.25             7.0   \n",
       "4                  0.90         563.5      318.5     122.50             7.0   \n",
       "\n",
       "   orientation  glazing_area  glazing_area_distribution  heating_load  \\\n",
       "0            2           0.0                          0         15.55   \n",
       "1            3           0.0                          0         15.55   \n",
       "2            4           0.0                          0         15.55   \n",
       "3            5           0.0                          0         15.55   \n",
       "4            2           0.0                          0         20.84   \n",
       "\n",
       "   cooling_load  \n",
       "0         21.33  \n",
       "1         21.33  \n",
       "2         21.33  \n",
       "3         21.33  \n",
       "4         28.28  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting Orientation and Glazing area distribution to one hot as they are categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "orientation_array = np.array(df[\"orientation\"])\n",
    "encoded = to_categorical(orientation_array)\n",
    "encoded_df = pd.DataFrame(encoded)\n",
    "orientation_encoded_df = encoded_df.drop([0,1], axis = 1)\n",
    "orientation_encoded_df.columns = [\"North\", \"East\", \"South\", \"West\"]\n",
    "\n",
    "glazing_area_disbn_array = np.array(df[\"glazing_area_distribution\"])\n",
    "encoded = to_categorical(glazing_area_disbn_array)\n",
    "glazing_area_disbn_encoded_df = pd.DataFrame(encoded)\n",
    "glazing_area_disbn_encoded_df.columns = [\"UnKnownGlaze\", \"UniformGlaze\", \"NorthGlaze\", \"EastGlaze\", \"SouthGlaze\", \"WestGlaze\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_df = pd.concat([df, orientation_encoded_df, glazing_area_disbn_encoded_df], axis=1)\n",
    "new_df = new_df.drop([\"orientation\",\"glazing_area_distribution\"], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['relative_compactness', 'surface_area', 'wall_area', 'roof_area',\n",
       "       'overall_height', 'glazing_area', 'heating_load', 'cooling_load',\n",
       "       'North', 'East', 'South', 'West', 'UnKnownGlaze', 'UniformGlaze',\n",
       "       'NorthGlaze', 'EastGlaze', 'SouthGlaze', 'WestGlaze'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_columns = ['relative_compactness', 'surface_area', 'wall_area', 'roof_area',\n",
    "       'overall_height', 'North', 'East',\n",
    "       'South', 'West', 'glazing_area', 'UnKnownGlaze',\n",
    "       'UniformGlaze', 'NorthGlaze', 'EastGlaze', 'SouthGlaze', 'WestGlaze']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>relative_compactness</th>\n",
       "      <th>surface_area</th>\n",
       "      <th>wall_area</th>\n",
       "      <th>roof_area</th>\n",
       "      <th>overall_height</th>\n",
       "      <th>glazing_area</th>\n",
       "      <th>heating_load</th>\n",
       "      <th>cooling_load</th>\n",
       "      <th>North</th>\n",
       "      <th>East</th>\n",
       "      <th>South</th>\n",
       "      <th>West</th>\n",
       "      <th>UnKnownGlaze</th>\n",
       "      <th>UniformGlaze</th>\n",
       "      <th>NorthGlaze</th>\n",
       "      <th>EastGlaze</th>\n",
       "      <th>SouthGlaze</th>\n",
       "      <th>WestGlaze</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.98</td>\n",
       "      <td>514.5</td>\n",
       "      <td>294.0</td>\n",
       "      <td>110.25</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.55</td>\n",
       "      <td>21.33</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.98</td>\n",
       "      <td>514.5</td>\n",
       "      <td>294.0</td>\n",
       "      <td>110.25</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.55</td>\n",
       "      <td>21.33</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.98</td>\n",
       "      <td>514.5</td>\n",
       "      <td>294.0</td>\n",
       "      <td>110.25</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.55</td>\n",
       "      <td>21.33</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.98</td>\n",
       "      <td>514.5</td>\n",
       "      <td>294.0</td>\n",
       "      <td>110.25</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.55</td>\n",
       "      <td>21.33</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.90</td>\n",
       "      <td>563.5</td>\n",
       "      <td>318.5</td>\n",
       "      <td>122.50</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.84</td>\n",
       "      <td>28.28</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   relative_compactness  surface_area  wall_area  roof_area  overall_height  \\\n",
       "0                  0.98         514.5      294.0     110.25             7.0   \n",
       "1                  0.98         514.5      294.0     110.25             7.0   \n",
       "2                  0.98         514.5      294.0     110.25             7.0   \n",
       "3                  0.98         514.5      294.0     110.25             7.0   \n",
       "4                  0.90         563.5      318.5     122.50             7.0   \n",
       "\n",
       "   glazing_area  heating_load  cooling_load  North  East  South  West  \\\n",
       "0           0.0         15.55         21.33    1.0   0.0    0.0   0.0   \n",
       "1           0.0         15.55         21.33    0.0   1.0    0.0   0.0   \n",
       "2           0.0         15.55         21.33    0.0   0.0    1.0   0.0   \n",
       "3           0.0         15.55         21.33    0.0   0.0    0.0   1.0   \n",
       "4           0.0         20.84         28.28    1.0   0.0    0.0   0.0   \n",
       "\n",
       "   UnKnownGlaze  UniformGlaze  NorthGlaze  EastGlaze  SouthGlaze  WestGlaze  \n",
       "0           1.0           0.0         0.0        0.0         0.0        0.0  \n",
       "1           1.0           0.0         0.0        0.0         0.0        0.0  \n",
       "2           1.0           0.0         0.0        0.0         0.0        0.0  \n",
       "3           1.0           0.0         0.0        0.0         0.0        0.0  \n",
       "4           1.0           0.0         0.0        0.0         0.0        0.0  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = new_df[feature_columns]\n",
    "y = df[['heating_load','cooling_load']]\n",
    "y_heat = df['heating_load']\n",
    "y_cool = df['cooling_load']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us calculate heating load and cooling load using various regressors <br>\n",
    "The R2-score combined for both heating and cooling load will be considered as the metric to evaluate since both are equally important"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the train and test data and scale them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Linear Regression** model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-squared score (training): 0.918\n",
      "R-squared score (test): 0.883\n"
     ]
    }
   ],
   "source": [
    "linreg = LinearRegression().fit(X_train_scaled, y_train)\n",
    "\n",
    "print('R-squared score (training): {:.3f}'\n",
    "     .format(linreg.score(X_train_scaled, y_train)))\n",
    "print('R-squared score (test): {:.3f}'\n",
    "     .format(linreg.score(X_test_scaled, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear Regressiong gives a dececnt performance but overfits the training data. Let us run **SVR** and see if we can do any better.<br>\n",
    "Grid Search was done on SVR but code was not included here as it took a lot of time and to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Kernel\n",
      "R-squared score (training): 0.910\n",
      "R-squared score (test): 0.876\n",
      "\n",
      "\n",
      "RBF Kernel\n",
      "R-squared score (training): 0.925\n",
      "R-squared score (test): 0.882\n"
     ]
    }
   ],
   "source": [
    "print('Linear Kernel')\n",
    "svr = SVR(kernel = 'linear', epsilon = 0.1, C = 10)\n",
    "mregsvr = MultiOutputRegressor(svr)\n",
    "mregsvr.fit(X_train_scaled, y_train)\n",
    "print('R-squared score (training): {:.3f}'\n",
    "     .format(mregsvr.score(X_train_scaled, y_train)))\n",
    "print('R-squared score (test): {:.3f}'\n",
    "     .format(mregsvr.score(X_test_scaled, y_test)))\n",
    "\n",
    "print('\\n')\n",
    "print('RBF Kernel')\n",
    "svr = SVR(kernel = 'rbf', gamma = 'auto', epsilon = 0.1, C = 100)\n",
    "mregsvr = MultiOutputRegressor(svr)\n",
    "mregsvr.fit(X_train_scaled, y_train)\n",
    "print('R-squared score (training): {:.3f}'\n",
    "     .format(mregsvr.score(X_train_scaled, y_train)))\n",
    "print('R-squared score (test): {:.3f}'\n",
    "     .format(mregsvr.score(X_test_scaled, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVR was trained with different values of C, gamma and epsilon and gave results similar to Linear Regression. Let us try **KNN Regressor**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-squared train score for 2 neighbors: 0.9320\n",
      "R-squared test score for 2 neighbors: 0.8045\n",
      "R-squared train score for 3 neighbors: 0.9280\n",
      "R-squared test score for 3 neighbors: 0.8242\n",
      "R-squared train score for 4 neighbors: 0.9197\n",
      "R-squared test score for 4 neighbors: 0.8285\n",
      "R-squared train score for 5 neighbors: 0.9126\n",
      "R-squared test score for 5 neighbors: 0.8173\n",
      "R-squared train score for 6 neighbors: 0.9052\n",
      "R-squared test score for 6 neighbors: 0.8095\n",
      "R-squared train score for 7 neighbors: 0.8968\n",
      "R-squared test score for 7 neighbors: 0.8034\n",
      "R-squared train score for 8 neighbors: 0.8899\n",
      "R-squared test score for 8 neighbors: 0.7978\n",
      "R-squared train score for 9 neighbors: 0.8815\n",
      "R-squared test score for 9 neighbors: 0.7971\n"
     ]
    }
   ],
   "source": [
    "for num_neighbors in range(2, 10):\n",
    "    knnreg = KNeighborsRegressor(n_neighbors = num_neighbors)\n",
    "    mregknn = MultiOutputRegressor(knnreg).fit(X_train_scaled, y_train)\n",
    "    \n",
    "    print('R-squared train score for {} neighbors: {:.4f}'\n",
    "         .format(num_neighbors, mregknn.score(X_train_scaled, y_train)))\n",
    "    print('R-squared test score for {} neighbors: {:.4f}'\n",
    "         .format(num_neighbors, mregknn.score(X_test_scaled, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNN is doing good for 5-8 neighbors but same as linear regression, overfitting the train data. Let us try **Decision Tree**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeRegressor(criterion='mse', max_depth=6, max_features=None,\n",
      "           max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "           min_impurity_split=None, min_samples_leaf=1,\n",
      "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "           presort=False, random_state=None, splitter='best')\n",
      "\n",
      "\n",
      "R-squared score (training): 0.988\n",
      "R-squared score (test): 0.972\n"
     ]
    }
   ],
   "source": [
    "param_grid = {'max_depth':[3,5,6,7,8]}\n",
    "\n",
    "grid_search = GridSearchCV(DecisionTreeRegressor(), param_grid, cv = 5).fit(X, y)\n",
    "print(grid_search.best_estimator_)\n",
    "\n",
    "dt = DecisionTreeRegressor(max_depth= 6).fit(X_train_scaled, y_train)\n",
    "\n",
    "print('\\n')\n",
    "print('R-squared score (training): {:.3f}'\n",
    "     .format(dt.score(X_train_scaled, y_train)))\n",
    "print('R-squared score (test): {:.3f}'\n",
    "     .format(dt.score(X_test_scaled, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Tree is giving good performance. Let us try **bagging and boosting this model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "R-squared score (training): 0.982\n",
      "R-squared score (test): 0.947\n",
      "\n",
      "\n",
      "R-squared score (training): 0.980\n",
      "R-squared score (test): 0.955\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y_cool, test_size=0.3, random_state=1)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "dt_base = DecisionTreeRegressor(max_depth= 6)\n",
    "dt = AdaBoostRegressor(dt_base, n_estimators = 300, random_state=1).fit(X_train_scaled, y_train)\n",
    "\n",
    "print('\\n')\n",
    "print('R-squared score (training): {:.3f}'\n",
    "     .format(dt.score(X_train_scaled, y_train)))\n",
    "print('R-squared score (test): {:.3f}'\n",
    "     .format(dt.score(X_test_scaled, y_test)))\n",
    "\n",
    "dt_base = DecisionTreeRegressor(max_depth= 6)\n",
    "dt = BaggingRegressor(dt_base, n_estimators = 300, random_state=1).fit(X_train_scaled, y_train)\n",
    "\n",
    "print('\\n')\n",
    "print('R-squared score (training): {:.3f}'\n",
    "     .format(dt.score(X_train_scaled, y_train)))\n",
    "print('R-squared score (test): {:.3f}'\n",
    "     .format(dt.score(X_test_scaled, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid Search on **Random Forest Regressor** to get best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=9,\n",
       "           max_features=15, max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "           min_impurity_split=None, min_samples_leaf=1,\n",
       "           min_samples_split=10, min_weight_fraction_leaf=0.0,\n",
       "           n_estimators=300, n_jobs=1, oob_score=False, random_state=1,\n",
       "           verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "param_grid = {'n_estimators':[200, 300, 500,1000],\n",
    "             'max_depth':[7, 9, 11, 13],\n",
    "             'min_samples_split':[5,10,20,40],\n",
    "             'max_features':[5,7,10,15]}\n",
    "\n",
    "rf_base = RandomForestRegressor(random_state=1)\n",
    "grid_search = GridSearchCV(rf_base, param_grid=param_grid, cv = 5).fit(X_scaled, y_cool)\n",
    "grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting random forest with best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-squared score (training): 0.988\n",
      "R-squared score (test): 0.977\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "rf = RandomForestRegressor(n_estimators=300, max_depth = 9, max_features=15, min_samples_split=10,\n",
    "                           min_samples_leaf = 1).fit(X_train_scaled, y_train)\n",
    "\n",
    "print('R-squared score (training): {:.3f}'\n",
    "     .format(rf.score(X_train_scaled, y_train)))\n",
    "print('R-squared score (test): {:.3f}'\n",
    "     .format(rf.score(X_test_scaled, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid Search on **Gradient Boosting Regressor** to get best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
       "             learning_rate=0.1, loss='ls', max_depth=7, max_features=15,\n",
       "             max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "             min_impurity_split=None, min_samples_leaf=1,\n",
       "             min_samples_split=40, min_weight_fraction_leaf=0.0,\n",
       "             n_estimators=1000, presort='auto', random_state=1,\n",
       "             subsample=1.0, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "param_grid = {'n_estimators':[200, 300, 500,1000],\n",
    "             'max_depth':[7, 9, 11, 13],\n",
    "             'min_samples_split':[5,10,20,40],\n",
    "             'max_features':[5,7,10,15]}\n",
    "\n",
    "gb_base = GradientBoostingRegressor(random_state=1)\n",
    "grid_search = GridSearchCV(gb_base, param_grid=param_grid, cv = 5).fit(X_scaled, y_cool)\n",
    "grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Gradient boosting with best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-squared score (training): 1.000\n",
      "R-squared score (test): 0.992\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "gb_base = GradientBoostingRegressor(n_estimators=1000, max_depth = 7, min_samples_leaf=1,\n",
    "                                    min_samples_split= 40, max_features=15)\n",
    "gb = MultiOutputRegressor(gb_base).fit(X_train_scaled, y_train)\n",
    "\n",
    "print('R-squared score (training): {:.3f}'\n",
    "     .format(gb.score(X_train_scaled, y_train)))\n",
    "print('R-squared score (test): {:.3f}'\n",
    "     .format(gb.score(X_test_scaled, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lasso and Ridge regression** on base line linear regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso regression: effect of alpha regularization\n",
      "parameter on number of features kept in final model\n",
      "\n",
      "Alpha = 0.005\n",
      "Features kept: 27, r-squared training: 0.917, r-squared test: 0.884\n",
      "\n",
      "Alpha = 0.010\n",
      "Features kept: 24, r-squared training: 0.916, r-squared test: 0.883\n",
      "\n",
      "Alpha = 0.100\n",
      "Features kept: 9, r-squared training: 0.906, r-squared test: 0.874\n",
      "\n",
      "Alpha = 0.200\n",
      "Features kept: 7, r-squared training: 0.896, r-squared test: 0.863\n",
      "\n",
      "Alpha = 0.500\n",
      "Features kept: 6, r-squared training: 0.840, r-squared test: 0.807\n",
      "\n",
      "Alpha = 1.000\n",
      "Features kept: 2, r-squared training: 0.763, r-squared test: 0.734\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Lasso regression: effect of alpha regularization\\n\\\n",
    "parameter on number of features kept in final model\\n')\n",
    "\n",
    "for alpha in [0.005, 0.01, 0.1, 0.2, 0.5, 1]:\n",
    "    linlasso = Lasso(alpha, max_iter = 10000).fit(X_train_scaled, y_train)\n",
    "    r2_train = linlasso.score(X_train_scaled, y_train)\n",
    "    r2_test = linlasso.score(X_test_scaled, y_test)\n",
    "    \n",
    "    print('Alpha = {:.3f}\\nFeatures kept: {}, r-squared training: {:.3f}, \\\n",
    "r-squared test: {:.3f}\\n'\n",
    "         .format(alpha, np.sum(linlasso.coef_ != 0), r2_train, r2_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lasso is not helping here. It is still overfitting the train data. Let us try Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge regression: Trying different values of alpha\n",
      "Alpha = 1.00\n",
      "num abs(coeff) > 1.0: 14, r-squared training: 0.91, r-squared test: 0.88\n",
      "\n",
      "Alpha = 10.00\n",
      "num abs(coeff) > 1.0: 12, r-squared training: 0.90, r-squared test: 0.86\n",
      "\n",
      "Alpha = 20.00\n",
      "num abs(coeff) > 1.0: 12, r-squared training: 0.89, r-squared test: 0.85\n",
      "\n",
      "Alpha = 50.00\n",
      "num abs(coeff) > 1.0: 12, r-squared training: 0.85, r-squared test: 0.81\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Ridge regression: Trying different values of alpha')\n",
    "for this_alpha in [1, 10, 20, 50]:\n",
    "    linRidge = Ridge(alpha=this_alpha).fit(X_train_scaled, y_train)\n",
    "    r2_train = linRidge.score(X_train_scaled, y_train)\n",
    "    r2_test = linRidge.score(X_test_scaled, y_test)\n",
    "    num_coeff_big = np.sum(abs(linRidge.coef_) > 1.0)\n",
    "    print('Alpha = {:.2f}\\nnum abs(coeff) > 1.0: {}, \\\n",
    "r-squared training: {:.2f}, r-squared test: {:.2f}\\n'\n",
    "         .format(this_alpha, num_coeff_big, r2_train, r2_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to Lasso Ridge is overfitting the train data for different values of alpha. Therefore these will not be considered for further analysis.<br>\n",
    "Let us try **polynomial features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "poly = PolynomialFeatures(degree=2)\n",
    "X_poly = poly.fit_transform(X)\n",
    "\n",
    "X_train_poly, X_test_poly, y_train, y_test = train_test_split(X_poly, y, test_size=0.3, random_state=1)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "X_train_poly_scaled = scaler.fit_transform(X_train_poly)\n",
    "X_test_poly_scaled = scaler.transform(X_test_poly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Polynomial Regression\n",
      "R-squared score (training): 0.989\n",
      "R-squared score (test): 0.982\n"
     ]
    }
   ],
   "source": [
    "linreg = LinearRegression().fit(X_train_poly_scaled, y_train)\n",
    "\n",
    "print('Polynomial Regression')\n",
    "print('R-squared score (training): {:.3f}'\n",
    "     .format(linreg.score(X_train_poly_scaled, y_train)))\n",
    "print('R-squared score (test): {:.3f}'\n",
    "     .format(linreg.score(X_test_poly_scaled, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Polynomial features is giving a great accuracy on both the train and test similar to decision trees. Let us run both Ridge and Lasso for these and check for any performance improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso regression: effect of alpha regularization\n",
      "parameter on number of features kept in final model\n",
      "\n",
      "Alpha = 0.0001\n",
      "Features kept: 170, r-squared training: 0.95, r-squared test: 0.93\n",
      "\n",
      "Alpha = 0.0010\n",
      "Features kept: 117, r-squared training: 0.95, r-squared test: 0.92\n",
      "\n",
      "Alpha = 0.0100\n",
      "Features kept: 58, r-squared training: 0.93, r-squared test: 0.90\n",
      "\n",
      "Alpha = 0.1000\n",
      "Features kept: 15, r-squared training: 0.92, r-squared test: 0.89\n",
      "\n",
      "Alpha = 0.2000\n",
      "Features kept: 12, r-squared training: 0.91, r-squared test: 0.88\n",
      "\n",
      "Alpha = 0.5000\n",
      "Features kept: 6, r-squared training: 0.86, r-squared test: 0.83\n",
      "\n",
      "Alpha = 1.0000\n",
      "Features kept: 5, r-squared training: 0.77, r-squared test: 0.74\n",
      "\n",
      "Ridge regression: Trying different values of alpha\n",
      "Alpha = 0.10\n",
      "num abs(coeff) > 1.0: 87, r-squared training: 0.94, r-squared test: 0.90\n",
      "\n",
      "Alpha = 1.00\n",
      "num abs(coeff) > 1.0: 50, r-squared training: 0.94, r-squared test: 0.89\n",
      "\n",
      "Alpha = 10.00\n",
      "num abs(coeff) > 1.0: 39, r-squared training: 0.93, r-squared test: 0.89\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Lasso regression: effect of alpha regularization\\n\\\n",
    "parameter on number of features kept in final model\\n')\n",
    "\n",
    "for alpha in [0.0001, 0.001, 0.01, 0.1, 0.2, 0.5, 1]:\n",
    "    linlasso = Lasso(alpha, max_iter = 10000).fit(X_train_poly_scaled, y_train)\n",
    "    r2_train = linlasso.score(X_train_poly_scaled, y_train)\n",
    "    r2_test = linlasso.score(X_test_poly_scaled, y_test)\n",
    "    \n",
    "    print('Alpha = {:.4f}\\nFeatures kept: {}, r-squared training: {:.2f}, \\\n",
    "r-squared test: {:.2f}\\n'\n",
    "         .format(alpha, np.sum(linlasso.coef_ != 0), r2_train, r2_test))\n",
    "    \n",
    "print('Ridge regression: Trying different values of alpha')\n",
    "for this_alpha in [0.1, 1, 10]:\n",
    "    linRidge = Ridge(alpha=this_alpha).fit(X_train_poly_scaled, y_train)\n",
    "    r2_train = linRidge.score(X_train_poly_scaled, y_train)\n",
    "    r2_test = linRidge.score(X_test_poly_scaled, y_test)\n",
    "    num_coeff_big = np.sum(abs(linRidge.coef_) > 1.0)\n",
    "    print('Alpha = {:.2f}\\nnum abs(coeff) > 1.0: {}, \\\n",
    "r-squared training: {:.2f}, r-squared test: {:.2f}\\n'\n",
    "         .format(this_alpha, num_coeff_big, r2_train, r2_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see both the Ridge and Lasso are hurting the penalty for of the Polynomial features. <br> We will for now try the<br> **1) Base line polynomial regression model<br>2) Random Forest<br> 3) Gradient Boosting<br> 4) Decision tree Regressor with bagging <br>5) Decision tree with boosting**<br> for calculating the heating load and cooling load seperately and see which is performing better on each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HEATING LOAD\n",
      "\n",
      "\n",
      "Polynomial Regression\n",
      "R-squared score (training): 0.998\n",
      "R-squared score (test): 0.998\n",
      "\n",
      "\n",
      "Decision Tree with Bagging\n",
      "R-squared score (training): 0.999\n",
      "R-squared score (test): 0.997\n",
      "\n",
      "\n",
      "Decision Tree with Boosting\n",
      "R-squared score (training): 0.998\n",
      "R-squared score (test): 0.997\n",
      "\n",
      "\n",
      "Random Forest Regressor\n",
      "R-squared score (training): 0.997\n",
      "R-squared score (test): 0.995\n",
      "\n",
      "\n",
      "Gradient Boosting Regressor\n",
      "R-squared score (training): 1.000\n",
      "R-squared score (test): 0.999\n",
      "\n",
      "\n",
      "COOLING LOAD\n",
      "\n",
      "\n",
      "Polynomial Regression\n",
      "R-squared score (training): 0.978\n",
      "R-squared score (test): 0.964\n",
      "\n",
      "\n",
      "Decision Tree with Bagging\n",
      "R-squared score (training): 0.985\n",
      "R-squared score (test): 0.955\n",
      "\n",
      "\n",
      "Decision Tree with Boosting\n",
      "R-squared score (training): 0.982\n",
      "R-squared score (test): 0.947\n",
      "\n",
      "\n",
      "Random Forest Regressor\n",
      "R-squared score (training): 0.980\n",
      "R-squared score (test): 0.956\n",
      "\n",
      "\n",
      "Gradient Boosting Regressor\n",
      "R-squared score (training): 1.000\n",
      "R-squared score (test): 0.986\n"
     ]
    }
   ],
   "source": [
    "print('HEATING LOAD')\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_heat, test_size=0.3, random_state=1)\n",
    "\n",
    "poly = PolynomialFeatures(degree=2)\n",
    "X_poly = poly.fit_transform(X)\n",
    "\n",
    "X_train_poly, X_test_poly, y_train, y_test = train_test_split(X_poly, y_heat, test_size=0.3, random_state=1)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "X_train_poly_scaled = scaler.fit_transform(X_train_poly)\n",
    "X_test_poly_scaled = scaler.transform(X_test_poly)\n",
    "linreg = LinearRegression().fit(X_train_poly_scaled, y_train)\n",
    "\n",
    "print('\\n')\n",
    "print('Polynomial Regression')\n",
    "print('R-squared score (training): {:.3f}'\n",
    "     .format(linreg.score(X_train_poly_scaled, y_train)))\n",
    "print('R-squared score (test): {:.3f}'\n",
    "     .format(linreg.score(X_test_poly_scaled, y_test)))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_heat, test_size=0.3, random_state=1)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "dt_base = DecisionTreeRegressor(max_depth= 7)\n",
    "dt = BaggingRegressor(dt_base, n_estimators = 300, random_state=1).fit(X_train_scaled, y_train)\n",
    "\n",
    "print('\\n')\n",
    "print('Decision Tree with Bagging')\n",
    "print('R-squared score (training): {:.3f}'\n",
    "     .format(dt.score(X_train_scaled, y_train)))\n",
    "print('R-squared score (test): {:.3f}'\n",
    "     .format(dt.score(X_test_scaled, y_test)))\n",
    "\n",
    "dt_base = DecisionTreeRegressor(max_depth= 6)\n",
    "dt = AdaBoostRegressor(dt_base, n_estimators = 300, random_state=1).fit(X_train_scaled, y_train)\n",
    "\n",
    "print('\\n')\n",
    "print('Decision Tree with Boosting')\n",
    "print('R-squared score (training): {:.3f}'\n",
    "     .format(dt.score(X_train_scaled, y_train)))\n",
    "print('R-squared score (test): {:.3f}'\n",
    "     .format(dt.score(X_test_scaled, y_test)))\n",
    "\n",
    "rf = RandomForestRegressor(n_estimators=300, max_depth = 9, max_features=15, min_samples_split=10,\n",
    "                           min_samples_leaf = 1).fit(X_train_scaled, y_train)\n",
    "\n",
    "print('\\n')\n",
    "print('Random Forest Regressor')\n",
    "print('R-squared score (training): {:.3f}'\n",
    "     .format(rf.score(X_train_scaled, y_train)))\n",
    "print('R-squared score (test): {:.3f}'\n",
    "     .format(rf.score(X_test_scaled, y_test)))\n",
    "\n",
    "gb_base = GradientBoostingRegressor(n_estimators=1000, max_depth = 7, min_samples_leaf=1,\n",
    "                                    min_samples_split= 40, max_features=15).fit(X_train_scaled, y_train)\n",
    "\n",
    "print('\\n')\n",
    "print('Gradient Boosting Regressor')\n",
    "print('R-squared score (training): {:.3f}'\n",
    "     .format(gb_base.score(X_train_scaled, y_train)))\n",
    "print('R-squared score (test): {:.3f}'\n",
    "     .format(gb_base.score(X_test_scaled, y_test)))\n",
    "\n",
    "print('\\n')\n",
    "print('COOLING LOAD')\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_cool, test_size=0.3, random_state=1)\n",
    "\n",
    "poly = PolynomialFeatures(degree=2)\n",
    "X_poly = poly.fit_transform(X)\n",
    "\n",
    "X_train_poly, X_test_poly, y_train, y_test = train_test_split(X_poly, y_cool, test_size=0.3, random_state=1)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "X_train_poly_scaled = scaler.fit_transform(X_train_poly)\n",
    "X_test_poly_scaled = scaler.transform(X_test_poly)\n",
    "\n",
    "linreg = LinearRegression().fit(X_train_poly_scaled, y_train)\n",
    "\n",
    "print('\\n')\n",
    "print('Polynomial Regression')\n",
    "print('R-squared score (training): {:.3f}'\n",
    "     .format(linreg.score(X_train_poly_scaled, y_train)))\n",
    "print('R-squared score (test): {:.3f}'\n",
    "     .format(linreg.score(X_test_poly_scaled, y_test)))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_cool, test_size=0.3, random_state=1)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "dt_base = DecisionTreeRegressor(max_depth= 7)\n",
    "dt = BaggingRegressor(dt_base, n_estimators = 300, random_state=1).fit(X_train_scaled, y_train)\n",
    "\n",
    "print('\\n')\n",
    "print('Decision Tree with Bagging')\n",
    "print('R-squared score (training): {:.3f}'\n",
    "     .format(dt.score(X_train_scaled, y_train)))\n",
    "print('R-squared score (test): {:.3f}'\n",
    "     .format(dt.score(X_test_scaled, y_test)))\n",
    "\n",
    "dt_base = DecisionTreeRegressor(max_depth= 6)\n",
    "dt = AdaBoostRegressor(dt_base, n_estimators = 300, random_state=1).fit(X_train_scaled, y_train)\n",
    "\n",
    "print('\\n')\n",
    "print('Decision Tree with Boosting')\n",
    "print('R-squared score (training): {:.3f}'\n",
    "     .format(dt.score(X_train_scaled, y_train)))\n",
    "print('R-squared score (test): {:.3f}'\n",
    "     .format(dt.score(X_test_scaled, y_test)))\n",
    "\n",
    "rf = RandomForestRegressor(n_estimators=300, max_depth = 9, max_features=15, min_samples_split=10,\n",
    "                           min_samples_leaf = 1).fit(X_train_scaled, y_train)\n",
    "\n",
    "print('\\n')\n",
    "print('Random Forest Regressor')\n",
    "print('R-squared score (training): {:.3f}'\n",
    "     .format(rf.score(X_train_scaled, y_train)))\n",
    "print('R-squared score (test): {:.3f}'\n",
    "     .format(rf.score(X_test_scaled, y_test)))\n",
    "\n",
    "gb_base = GradientBoostingRegressor(n_estimators=1000, max_depth = 7, min_samples_leaf=1,\n",
    "                                    min_samples_split= 40, max_features=15).fit(X_train_scaled, y_train)\n",
    "\n",
    "print('\\n')\n",
    "print('Gradient Boosting Regressor')\n",
    "print('R-squared score (training): {:.3f}'\n",
    "     .format(gb_base.score(X_train_scaled, y_train)))\n",
    "print('R-squared score (test): {:.3f}'\n",
    "     .format(gb_base.score(X_test_scaled, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the results that **Gradient Boosting** is performing the best for both heating and cooling loads.<br>\n",
    "The model is<br>\n",
    "**GradientBoostingRegressor(\n",
    "alpha=0.9, criterion='friedman_mse', init=None,<br>\n",
    "learning_rate=0.1, loss='ls', max_depth=7, max_features=15,<br>\n",
    "max_leaf_nodes=None, min_impurity_decrease=0.0,<br>\n",
    "min_impurity_split=None, min_samples_leaf=1,<br>\n",
    "min_samples_split=40, min_weight_fraction_leaf=0.0,<br>\n",
    "n_estimators=1000, presort='auto', random_state=1,<br>\n",
    "subsample=1.0, verbose=0, warm_start=False<br>\n",
    ")**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DEEP LEARNING\n",
    "to compute overall load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x290b2559da0>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAENCAYAAAAPAhLDAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x290b2563c88>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df['overall_load'] = df['heating_load'] + df['cooling_load']\n",
    "sns.distplot(df['overall_load'], bins=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x290b2563978>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAENCAYAAADjW7WQAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIf8fE1GS3/25IHIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x290b0268748>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.distplot(df['heating_load'], bins=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x290b029d0f0>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAENCAYAAADjW7WQAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x290b2eec630>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.distplot(df['cooling_load'], bins=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see from the plots that using pd.cut work completely fine as the overall load has a neat distribution. Also, the heating load and cooling load also have almost same distribution like the overall load."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['load_category'] = pd.cut(df['overall_load'], 3, labels=[\"low_efficient\", \"average_efficient\", \"high_efficient\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Grid Search was tried but we were unable to run that. So, we have written manual for loops to find the best hyper parameters and tune them<br>\n",
    "Also the runs were giving different best parameters every time they ran so we have chosen for the iteration we did. The result might differ when you run the notebook again for best parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting target to a 3D tensor and scaling the data first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = new_df[feature_columns]\n",
    "y = df['load_category']\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(y)\n",
    "encoded_Y = encoder.transform(y)\n",
    "dummy_y = np_utils.to_categorical(encoded_Y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,dummy_y, test_size=0.3, random_state=1, shuffle=True)\n",
    "\n",
    "np.random.seed(1)\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding the best values for **epochs and batch size**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "537/537 [==============================] - 0s 51us/step\n",
      "231/231 [==============================] - 0s 35us/step\n",
      "\n",
      "\n",
      "Batch size: 5\n",
      "epochs: 100\n",
      "Training acc: 91.06%\n",
      "Testing acc: 88.31%\n",
      "537/537 [==============================] - 0s 22us/step\n",
      "231/231 [==============================] - 0s 41us/step\n",
      "\n",
      "\n",
      "Batch size: 10\n",
      "epochs: 100\n",
      "Training acc: 92.92%\n",
      "Testing acc: 89.18%\n",
      "537/537 [==============================] - 0s 19us/step\n",
      "231/231 [==============================] - 0s 41us/step\n",
      "\n",
      "\n",
      "Batch size: 15\n",
      "epochs: 100\n",
      "Training acc: 93.67%\n",
      "Testing acc: 89.61%\n",
      "537/537 [==============================] - 0s 21us/step\n",
      "231/231 [==============================] - 0s 33us/step\n",
      "\n",
      "\n",
      "Batch size: 20\n",
      "epochs: 100\n",
      "Training acc: 93.67%\n",
      "Testing acc: 89.18%\n",
      "537/537 [==============================] - 0s 20us/step\n",
      "231/231 [==============================] - 0s 41us/step\n",
      "\n",
      "\n",
      "Batch size: 25\n",
      "epochs: 100\n",
      "Training acc: 93.48%\n",
      "Testing acc: 89.18%\n",
      "537/537 [==============================] - 0s 23us/step\n",
      "231/231 [==============================] - 0s 39us/step\n",
      "\n",
      "\n",
      "Batch size: 5\n",
      "epochs: 200\n",
      "Training acc: 93.30%\n",
      "Testing acc: 89.18%\n",
      "537/537 [==============================] - 0s 23us/step\n",
      "231/231 [==============================] - 0s 33us/step\n",
      "\n",
      "\n",
      "Batch size: 10\n",
      "epochs: 200\n",
      "Training acc: 93.48%\n",
      "Testing acc: 90.04%\n",
      "537/537 [==============================] - 0s 20us/step\n",
      "231/231 [==============================] - 0s 41us/step\n",
      "\n",
      "\n",
      "Batch size: 15\n",
      "epochs: 200\n",
      "Training acc: 94.41%\n",
      "Testing acc: 90.48%\n",
      "537/537 [==============================] - 0s 23us/step\n",
      "231/231 [==============================] - 0s 33us/step\n",
      "\n",
      "\n",
      "Batch size: 20\n",
      "epochs: 200\n",
      "Training acc: 94.79%\n",
      "Testing acc: 90.48%\n",
      "537/537 [==============================] - 0s 21us/step\n",
      "231/231 [==============================] - 0s 35us/step\n",
      "\n",
      "\n",
      "Batch size: 25\n",
      "epochs: 200\n",
      "Training acc: 95.16%\n",
      "Testing acc: 90.48%\n",
      "537/537 [==============================] - 0s 22us/step\n",
      "231/231 [==============================] - 0s 33us/step\n",
      "\n",
      "\n",
      "Batch size: 5\n",
      "epochs: 300\n",
      "Training acc: 94.23%\n",
      "Testing acc: 89.61%\n",
      "537/537 [==============================] - 0s 20us/step\n",
      "231/231 [==============================] - 0s 43us/step\n",
      "\n",
      "\n",
      "Batch size: 10\n",
      "epochs: 300\n",
      "Training acc: 94.79%\n",
      "Testing acc: 90.48%\n",
      "537/537 [==============================] - 0s 24us/step\n",
      "231/231 [==============================] - 0s 37us/step\n",
      "\n",
      "\n",
      "Batch size: 15\n",
      "epochs: 300\n",
      "Training acc: 95.34%\n",
      "Testing acc: 90.48%\n",
      "537/537 [==============================] - 0s 22us/step\n",
      "231/231 [==============================] - 0s 41us/step\n",
      "\n",
      "\n",
      "Batch size: 20\n",
      "epochs: 300\n",
      "Training acc: 95.90%\n",
      "Testing acc: 90.04%\n",
      "537/537 [==============================] - 0s 22us/step\n",
      "231/231 [==============================] - 0s 33us/step\n",
      "\n",
      "\n",
      "Batch size: 25\n",
      "epochs: 300\n",
      "Training acc: 96.09%\n",
      "Testing acc: 90.48%\n",
      "537/537 [==============================] - 0s 23us/step\n",
      "231/231 [==============================] - 0s 30us/step\n",
      "\n",
      "\n",
      "Batch size: 5\n",
      "epochs: 350\n",
      "Training acc: 96.28%\n",
      "Testing acc: 90.91%\n",
      "537/537 [==============================] - 0s 22us/step\n",
      "231/231 [==============================] - 0s 43us/step\n",
      "\n",
      "\n",
      "Batch size: 10\n",
      "epochs: 350\n",
      "Training acc: 95.90%\n",
      "Testing acc: 90.04%\n",
      "537/537 [==============================] - 0s 23us/step\n",
      "231/231 [==============================] - 0s 46us/step\n",
      "\n",
      "\n",
      "Batch size: 15\n",
      "epochs: 350\n",
      "Training acc: 96.09%\n",
      "Testing acc: 90.04%\n",
      "537/537 [==============================] - 0s 23us/step\n",
      "231/231 [==============================] - 0s 35us/step\n",
      "\n",
      "\n",
      "Batch size: 20\n",
      "epochs: 350\n",
      "Training acc: 96.09%\n",
      "Testing acc: 90.48%\n",
      "537/537 [==============================] - 0s 19us/step\n",
      "231/231 [==============================] - 0s 41us/step\n",
      "\n",
      "\n",
      "Batch size: 25\n",
      "epochs: 350\n",
      "Training acc: 96.28%\n",
      "Testing acc: 89.61%\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "model = Sequential()\n",
    "model.add(Dense(8, input_dim=16, activation='relu'))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "for epochs in [100,200,300,350]:\n",
    "    for batch_size in [5,10,15,20,25]:\n",
    "        seed = 1\n",
    "        np.random.seed(seed)\n",
    "        model.fit(X_train_scaled, y_train, epochs=epochs, batch_size=batch_size, verbose=0)\n",
    "\n",
    "        scores_train = model.evaluate(X_train_scaled, y_train)\n",
    "        scores_test = model.evaluate(X_test_scaled, y_test)\n",
    "\n",
    "        print('\\n')\n",
    "        print('Batch size: ',end=\"\")\n",
    "        print(batch_size)\n",
    "        print('epochs: ',end=\"\")\n",
    "        print(epochs)\n",
    "        print('Training ',end=\"\")\n",
    "        print(\"%s: %.2f%%\" % (model.metrics_names[1], scores_train[1]*100))\n",
    "        print('Testing ',end=\"\")\n",
    "        print(\"%s: %.2f%%\" % (model.metrics_names[1], scores_test[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best number of epochs are 200 and batch size 20. These will be continued further <br>\n",
    "Next, Finding the **best optimizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "537/537 [==============================] - 0s 70us/step\n",
      "231/231 [==============================] - 0s 43us/step\n",
      "\n",
      "\n",
      "Optimizer: SGD\n",
      "Training acc: 89.20%\n",
      "Testing acc: 86.15%\n",
      "537/537 [==============================] - 0s 68us/step\n",
      "231/231 [==============================] - 0s 46us/step\n",
      "\n",
      "\n",
      "Optimizer: RMSprop\n",
      "Training acc: 91.43%\n",
      "Testing acc: 88.31%\n",
      "537/537 [==============================] - 0s 88us/step\n",
      "231/231 [==============================] - 0s 35us/step\n",
      "\n",
      "\n",
      "Optimizer: Adagrad\n",
      "Training acc: 92.36%\n",
      "Testing acc: 88.31%\n",
      "537/537 [==============================] - 0s 93us/step\n",
      "231/231 [==============================] - 0s 37us/step\n",
      "\n",
      "\n",
      "Optimizer: Adadelta\n",
      "Training acc: 92.92%\n",
      "Testing acc: 89.18%\n",
      "537/537 [==============================] - 0s 107us/step\n",
      "231/231 [==============================] - 0s 43us/step\n",
      "\n",
      "\n",
      "Optimizer: Adam\n",
      "Training acc: 92.92%\n",
      "Testing acc: 88.74%\n",
      "537/537 [==============================] - 0s 124us/step\n",
      "231/231 [==============================] - 0s 35us/step\n",
      "\n",
      "\n",
      "Optimizer: Adamax\n",
      "Training acc: 93.48%\n",
      "Testing acc: 89.18%\n",
      "537/537 [==============================] - 0s 138us/step\n",
      "231/231 [==============================] - 0s 41us/step\n",
      "\n",
      "\n",
      "Optimizer: Nadam\n",
      "Training acc: 94.60%\n",
      "Testing acc: 90.04%\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "model = Sequential()\n",
    "model.add(Dense(8, input_dim=16, activation='relu'))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "for optim in ['SGD', 'RMSprop', 'Adagrad', 'Adadelta', 'Adam', 'Adamax', 'Nadam']:\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optim, metrics=['accuracy'])\n",
    "    seed = 1\n",
    "    np.random.seed(seed)\n",
    "    model.fit(X_train_scaled, y_train, epochs=200, batch_size=20, verbose=0)\n",
    "\n",
    "    scores_train = model.evaluate(X_train_scaled, y_train)\n",
    "    scores_test = model.evaluate(X_test_scaled, y_test)\n",
    "\n",
    "    print('\\n')\n",
    "    print('Optimizer: ',end=\"\")\n",
    "    print(optim)\n",
    "    print('Training ', end=\"\")\n",
    "    print(\"%s: %.2f%%\" % (model.metrics_names[1], scores_train[1]*100))\n",
    "    print('Testing ', end=\"\")\n",
    "    print(\"%s: %.2f%%\" % (model.metrics_names[1], scores_test[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best optimizer is Nadam (Nesterov Adam optimizer). This wil be continued further<br>\n",
    "Next, Finding the best value for **learning rate**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "537/537 [==============================] - 0s 159us/step\n",
      "231/231 [==============================] - 0s 50us/step\n",
      "\n",
      "\n",
      "learn rate: 0.001\n",
      "Training acc: 92.74%\n",
      "Testing acc: 88.74%\n",
      "537/537 [==============================] - 0s 168us/step\n",
      "231/231 [==============================] - 0s 41us/step\n",
      "\n",
      "\n",
      "learn rate: 0.002\n",
      "Training acc: 93.67%\n",
      "Testing acc: 89.18%\n",
      "537/537 [==============================] - 0s 192us/step\n",
      "231/231 [==============================] - 0s 46us/step\n",
      "\n",
      "\n",
      "learn rate: 0.005\n",
      "Training acc: 94.04%\n",
      "Testing acc: 90.04%\n",
      "537/537 [==============================] - 0s 206us/step\n",
      "231/231 [==============================] - 0s 39us/step\n",
      "\n",
      "\n",
      "learn rate: 0.01\n",
      "Training acc: 95.72%\n",
      "Testing acc: 89.18%\n",
      "537/537 [==============================] - 0s 225us/step\n",
      "231/231 [==============================] - 0s 43us/step\n",
      "\n",
      "\n",
      "learn rate: 0.1\n",
      "Training acc: 97.39%\n",
      "Testing acc: 93.07%\n",
      "537/537 [==============================] - 0s 235us/step\n",
      "231/231 [==============================] - 0s 46us/step\n",
      "\n",
      "\n",
      "learn rate: 1\n",
      "Training acc: 74.86%\n",
      "Testing acc: 71.00%\n",
      "537/537 [==============================] - 0s 258us/step\n",
      "231/231 [==============================] - 0s 35us/step\n",
      "\n",
      "\n",
      "learn rate: 10\n",
      "Training acc: 74.86%\n",
      "Testing acc: 71.00%\n"
     ]
    }
   ],
   "source": [
    "from keras.optimizers import Nadam\n",
    "\n",
    "np.random.seed(1)\n",
    "model = Sequential()\n",
    "model.add(Dense(8, input_dim=16, activation='relu'))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "for learn_rate in [0.001,0.002, 0.005, 0.01, 0.1, 1, 10]:\n",
    "    optim = Nadam(lr = learn_rate)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optim, metrics=['accuracy'])\n",
    "\n",
    "    seed = 1\n",
    "    np.random.seed(seed)\n",
    "    model.fit(X_train_scaled, y_train, epochs=200, batch_size=20, verbose=0)\n",
    "\n",
    "    scores_train = model.evaluate(X_train_scaled, y_train)\n",
    "    scores_test = model.evaluate(X_test_scaled, y_test)\n",
    "\n",
    "    print('\\n')\n",
    "    print('learn rate: ',end=\"\")\n",
    "    print(learn_rate)\n",
    "    print('Training ', end=\"\")\n",
    "    print(\"%s: %.2f%%\" % (model.metrics_names[1], scores_train[1]*100))\n",
    "    print('Testing ', end=\"\")\n",
    "    print(\"%s: %.2f%%\" % (model.metrics_names[1], scores_test[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best learning rate is 0.1. This will be continued further. <br>\n",
    "Next, Finding the best value for **initialization mode**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "537/537 [==============================] - 0s 285us/step\n",
      "231/231 [==============================] - 0s 48us/step\n",
      "\n",
      "\n",
      "init mode: uniform\n",
      "Training acc: 95.16%\n",
      "Testing acc: 93.51%\n",
      "537/537 [==============================] - 0s 296us/step\n",
      "231/231 [==============================] - 0s 45us/step\n",
      "\n",
      "\n",
      "init mode: lecun_uniform\n",
      "Training acc: 97.21%\n",
      "Testing acc: 94.37%\n",
      "537/537 [==============================] - 0s 316us/step\n",
      "231/231 [==============================] - 0s 39us/step\n",
      "\n",
      "\n",
      "init mode: normal\n",
      "Training acc: 91.99%\n",
      "Testing acc: 88.74%\n",
      "537/537 [==============================] - 0s 333us/step\n",
      "231/231 [==============================] - 0s 46us/step\n",
      "\n",
      "\n",
      "init mode: zero\n",
      "Training acc: 51.96%\n",
      "Testing acc: 46.75%\n",
      "537/537 [==============================] - 0s 359us/step\n",
      "231/231 [==============================] - 0s 39us/step\n",
      "\n",
      "\n",
      "init mode: glorot_normal\n",
      "Training acc: 91.62%\n",
      "Testing acc: 90.04%\n",
      "537/537 [==============================] - 0s 373us/step\n",
      "231/231 [==============================] - 0s 43us/step\n",
      "\n",
      "\n",
      "init mode: glorot_uniform\n",
      "Training acc: 96.65%\n",
      "Testing acc: 94.37%\n",
      "537/537 [==============================] - 0s 401us/step\n",
      "231/231 [==============================] - 0s 37us/step\n",
      "\n",
      "\n",
      "init mode: he_normal\n",
      "Training acc: 95.53%\n",
      "Testing acc: 90.04%\n",
      "537/537 [==============================] - 0s 411us/step\n",
      "231/231 [==============================] - 0s 48us/step\n",
      "\n",
      "\n",
      "init mode: he_uniform\n",
      "Training acc: 93.48%\n",
      "Testing acc: 90.04%\n"
     ]
    }
   ],
   "source": [
    "from keras.optimizers import Nadam\n",
    "\n",
    "for init_mode in ['uniform', 'lecun_uniform', 'normal', 'zero', 'glorot_normal', 'glorot_uniform', 'he_normal', 'he_uniform']:\n",
    "    \n",
    "    np.random.seed(1)\n",
    "    model = Sequential()\n",
    "    model.add(Dense(8, input_dim=16, kernel_initializer=init_mode, activation='relu'))\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "    optim = Nadam(lr = 0.1)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optim, metrics=['accuracy'])\n",
    "\n",
    "    seed = 1\n",
    "    np.random.seed(seed)\n",
    "    model.fit(X_train_scaled, y_train, epochs=200, batch_size=20, verbose=0)\n",
    "\n",
    "    scores_train = model.evaluate(X_train_scaled, y_train)\n",
    "    scores_test = model.evaluate(X_test_scaled, y_test)\n",
    "\n",
    "    print('\\n')\n",
    "    print('init mode: ',end=\"\")\n",
    "    print(init_mode)\n",
    "    print('Training ', end=\"\")\n",
    "    print(\"%s: %.2f%%\" % (model.metrics_names[1], scores_train[1]*100))\n",
    "    print('Testing ', end=\"\")\n",
    "    print(\"%s: %.2f%%\" % (model.metrics_names[1], scores_test[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best kernel initializer for the hidden layer is from a He normal distribution. This will be continued further. <br>\n",
    "Next, Finding the best value for **activation function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "537/537 [==============================] - 0s 432us/step\n",
      "231/231 [==============================] - 0s 39us/step\n",
      "\n",
      "\n",
      "activation: softmax\n",
      "Training acc: 95.53%\n",
      "Testing acc: 91.34%\n",
      "537/537 [==============================] - 0s 446us/step\n",
      "231/231 [==============================] - 0s 43us/step\n",
      "\n",
      "\n",
      "activation: softplus\n",
      "Training acc: 96.46%\n",
      "Testing acc: 90.04%\n",
      "537/537 [==============================] - 0s 466us/step\n",
      "231/231 [==============================] - 0s 39us/step\n",
      "\n",
      "\n",
      "activation: softsign\n",
      "Training acc: 100.00%\n",
      "Testing acc: 90.91%\n",
      "537/537 [==============================] - 0s 481us/step\n",
      "231/231 [==============================] - 0s 46us/step\n",
      "\n",
      "\n",
      "activation: relu\n",
      "Training acc: 95.90%\n",
      "Testing acc: 91.34%\n",
      "537/537 [==============================] - 0s 498us/step\n",
      "231/231 [==============================] - 0s 41us/step\n",
      "\n",
      "\n",
      "activation: tanh\n",
      "Training acc: 98.51%\n",
      "Testing acc: 92.64%\n",
      "537/537 [==============================] - 0s 520us/step\n",
      "231/231 [==============================] - 0s 50us/step\n",
      "\n",
      "\n",
      "activation: sigmoid\n",
      "Training acc: 96.28%\n",
      "Testing acc: 89.18%\n",
      "537/537 [==============================] - 0s 545us/step\n",
      "231/231 [==============================] - 0s 48us/step\n",
      "\n",
      "\n",
      "activation: hard_sigmoid\n",
      "Training acc: 99.63%\n",
      "Testing acc: 91.77%\n",
      "537/537 [==============================] - 0s 556us/step\n",
      "231/231 [==============================] - 0s 46us/step\n",
      "\n",
      "\n",
      "activation: linear\n",
      "Training acc: 90.50%\n",
      "Testing acc: 89.18%\n"
     ]
    }
   ],
   "source": [
    "from keras.optimizers import Nadam\n",
    "\n",
    "for activation in ['softmax', 'softplus', 'softsign', 'relu', 'tanh', 'sigmoid', 'hard_sigmoid', 'linear']:\n",
    "    \n",
    "    np.random.seed(1)\n",
    "    model = Sequential()\n",
    "    model.add(Dense(8, input_dim=16, kernel_initializer='he_normal', activation=activation))\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "    optim = Nadam(lr = 0.1)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optim, metrics=['accuracy'])\n",
    "\n",
    "    seed = 1\n",
    "    np.random.seed(seed)\n",
    "    model.fit(X_train_scaled, y_train, epochs=200, batch_size=20, verbose=0)\n",
    "\n",
    "    scores_train = model.evaluate(X_train_scaled, y_train)\n",
    "    scores_test = model.evaluate(X_test_scaled, y_test)\n",
    "\n",
    "    print('\\n')\n",
    "    print('activation: ',end=\"\")\n",
    "    print(activation)\n",
    "    print('Training ', end=\"\")\n",
    "    print(\"%s: %.2f%%\" % (model.metrics_names[1], scores_train[1]*100))\n",
    "    print('Testing ', end=\"\")\n",
    "    print(\"%s: %.2f%%\" % (model.metrics_names[1], scores_test[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best activation funciton is softplus. This will be continued further. <br>\n",
    "Next, Finding the best values for **Drop rate in drop out regularization and weight constraint**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "537/537 [==============================] - 0s 575us/step\n",
      "231/231 [==============================] - 0s 50us/step\n",
      "\n",
      "\n",
      "Drop rate: 0.0\n",
      "Weight Constraint: 1\n",
      "Training acc: 91.43%\n",
      "Testing acc: 88.74%\n",
      "537/537 [==============================] - 0s 598us/step\n",
      "231/231 [==============================] - 0s 52us/step\n",
      "\n",
      "\n",
      "Drop rate: 0.0\n",
      "Weight Constraint: 2\n",
      "Training acc: 92.18%\n",
      "Testing acc: 89.18%\n",
      "537/537 [==============================] - 0s 621us/step\n",
      "231/231 [==============================] - 0s 50us/step\n",
      "\n",
      "\n",
      "Drop rate: 0.0\n",
      "Weight Constraint: 3\n",
      "Training acc: 93.67%\n",
      "Testing acc: 89.61%\n",
      "537/537 [==============================] - 0s 632us/step\n",
      "231/231 [==============================] - 0s 44us/step\n",
      "\n",
      "\n",
      "Drop rate: 0.0\n",
      "Weight Constraint: 4\n",
      "Training acc: 94.79%\n",
      "Testing acc: 92.21%\n",
      "537/537 [==============================] - 0s 669us/step\n",
      "231/231 [==============================] - 0s 48us/step\n",
      "\n",
      "\n",
      "Drop rate: 0.0\n",
      "Weight Constraint: 5\n",
      "Training acc: 95.72%\n",
      "Testing acc: 92.21%\n",
      "537/537 [==============================] - 0s 682us/step\n",
      "231/231 [==============================] - 0s 45us/step\n",
      "\n",
      "\n",
      "Drop rate: 0.1\n",
      "Weight Constraint: 1\n",
      "Training acc: 90.32%\n",
      "Testing acc: 86.15%\n",
      "537/537 [==============================] - 0s 777us/step\n",
      "231/231 [==============================] - 0s 43us/step\n",
      "\n",
      "\n",
      "Drop rate: 0.1\n",
      "Weight Constraint: 2\n",
      "Training acc: 90.32%\n",
      "Testing acc: 87.88%\n",
      "537/537 [==============================] - 0s 723us/step\n",
      "231/231 [==============================] - 0s 50us/step\n",
      "\n",
      "\n",
      "Drop rate: 0.1\n",
      "Weight Constraint: 3\n",
      "Training acc: 91.99%\n",
      "Testing acc: 88.31%\n",
      "537/537 [==============================] - 1s 969us/step\n",
      "231/231 [==============================] - 0s 59us/step\n",
      "\n",
      "\n",
      "Drop rate: 0.1\n",
      "Weight Constraint: 4\n",
      "Training acc: 93.11%\n",
      "Testing acc: 90.04%\n",
      "537/537 [==============================] - 1s 941us/step\n",
      "231/231 [==============================] - 0s 67us/step\n",
      "\n",
      "\n",
      "Drop rate: 0.1\n",
      "Weight Constraint: 5\n",
      "Training acc: 93.11%\n",
      "Testing acc: 90.04%\n",
      "537/537 [==============================] - 0s 813us/step\n",
      "231/231 [==============================] - 0s 48us/step\n",
      "\n",
      "\n",
      "Drop rate: 0.2\n",
      "Weight Constraint: 1\n",
      "Training acc: 89.94%\n",
      "Testing acc: 85.28%\n",
      "537/537 [==============================] - 0s 828us/step\n",
      "231/231 [==============================] - 0s 48us/step\n",
      "\n",
      "\n",
      "Drop rate: 0.2\n",
      "Weight Constraint: 2\n",
      "Training acc: 90.13%\n",
      "Testing acc: 87.01%\n",
      "537/537 [==============================] - 0s 857us/step\n",
      "231/231 [==============================] - 0s 41us/step\n",
      "\n",
      "\n",
      "Drop rate: 0.2\n",
      "Weight Constraint: 3\n",
      "Training acc: 90.88%\n",
      "Testing acc: 87.88%\n",
      "537/537 [==============================] - 0s 878us/step\n",
      "231/231 [==============================] - 0s 47us/step\n",
      "\n",
      "\n",
      "Drop rate: 0.2\n",
      "Weight Constraint: 4\n",
      "Training acc: 92.55%\n",
      "Testing acc: 89.18%\n",
      "537/537 [==============================] - 0s 902us/step\n",
      "231/231 [==============================] - 0s 54us/step\n",
      "\n",
      "\n",
      "Drop rate: 0.2\n",
      "Weight Constraint: 5\n",
      "Training acc: 92.92%\n",
      "Testing acc: 89.61%\n",
      "537/537 [==============================] - 0s 916us/step\n",
      "231/231 [==============================] - 0s 61us/step\n",
      "\n",
      "\n",
      "Drop rate: 0.3\n",
      "Weight Constraint: 1\n",
      "Training acc: 89.57%\n",
      "Testing acc: 84.42%\n",
      "537/537 [==============================] - 0s 928us/step\n",
      "231/231 [==============================] - 0s 59us/step\n",
      "\n",
      "\n",
      "Drop rate: 0.3\n",
      "Weight Constraint: 2\n",
      "Training acc: 90.32%\n",
      "Testing acc: 87.01%\n",
      "537/537 [==============================] - 1s 966us/step\n",
      "231/231 [==============================] - 0s 56us/step\n",
      "\n",
      "\n",
      "Drop rate: 0.3\n",
      "Weight Constraint: 3\n",
      "Training acc: 90.32%\n",
      "Testing acc: 87.01%\n",
      "537/537 [==============================] - 1s 975us/step\n",
      "231/231 [==============================] - 0s 52us/step\n",
      "\n",
      "\n",
      "Drop rate: 0.3\n",
      "Weight Constraint: 4\n",
      "Training acc: 90.69%\n",
      "Testing acc: 87.45%\n",
      "537/537 [==============================] - 1s 1ms/step\n",
      "231/231 [==============================] - 0s 56us/step\n",
      "\n",
      "\n",
      "Drop rate: 0.3\n",
      "Weight Constraint: 5\n",
      "Training acc: 90.69%\n",
      "Testing acc: 88.31%\n",
      "537/537 [==============================] - 1s 1ms/step\n",
      "231/231 [==============================] - 0s 54us/step\n",
      "\n",
      "\n",
      "Drop rate: 0.4\n",
      "Weight Constraint: 1\n",
      "Training acc: 88.83%\n",
      "Testing acc: 83.12%\n",
      "537/537 [==============================] - 1s 1ms/step\n",
      "231/231 [==============================] - 0s 48us/step\n",
      "\n",
      "\n",
      "Drop rate: 0.4\n",
      "Weight Constraint: 2\n",
      "Training acc: 89.39%\n",
      "Testing acc: 86.58%\n",
      "537/537 [==============================] - 1s 1ms/step\n",
      "231/231 [==============================] - 0s 46us/step\n",
      "\n",
      "\n",
      "Drop rate: 0.4\n",
      "Weight Constraint: 3\n",
      "Training acc: 89.76%\n",
      "Testing acc: 87.45%\n",
      "537/537 [==============================] - 1s 1ms/step\n",
      "231/231 [==============================] - 0s 52us/step\n",
      "\n",
      "\n",
      "Drop rate: 0.4\n",
      "Weight Constraint: 4\n",
      "Training acc: 90.88%\n",
      "Testing acc: 86.58%\n",
      "537/537 [==============================] - 1s 1ms/step\n",
      "231/231 [==============================] - 0s 57us/step\n",
      "\n",
      "\n",
      "Drop rate: 0.4\n",
      "Weight Constraint: 5\n",
      "Training acc: 91.06%\n",
      "Testing acc: 87.01%\n",
      "537/537 [==============================] - 1s 1ms/step\n",
      "231/231 [==============================] - 0s 48us/step\n",
      "\n",
      "\n",
      "Drop rate: 0.5\n",
      "Weight Constraint: 1\n",
      "Training acc: 78.03%\n",
      "Testing acc: 73.59%\n",
      "537/537 [==============================] - 1s 1ms/step\n",
      "231/231 [==============================] - 0s 56us/step\n",
      "\n",
      "\n",
      "Drop rate: 0.5\n",
      "Weight Constraint: 2\n",
      "Training acc: 89.76%\n",
      "Testing acc: 85.28%\n",
      "537/537 [==============================] - 1s 1ms/step\n",
      "231/231 [==============================] - 0s 52us/step\n",
      "\n",
      "\n",
      "Drop rate: 0.5\n",
      "Weight Constraint: 3\n",
      "Training acc: 89.94%\n",
      "Testing acc: 85.71%\n",
      "537/537 [==============================] - 1s 1ms/step\n",
      "231/231 [==============================] - 0s 59us/step\n",
      "\n",
      "\n",
      "Drop rate: 0.5\n",
      "Weight Constraint: 4\n",
      "Training acc: 90.69%\n",
      "Testing acc: 87.01%\n",
      "537/537 [==============================] - 1s 1ms/step\n",
      "231/231 [==============================] - 0s 52us/step\n",
      "\n",
      "\n",
      "Drop rate: 0.5\n",
      "Weight Constraint: 5\n",
      "Training acc: 90.69%\n",
      "Testing acc: 87.45%\n",
      "537/537 [==============================] - 1s 1ms/step\n",
      "231/231 [==============================] - 0s 48us/step\n",
      "\n",
      "\n",
      "Drop rate: 0.6\n",
      "Weight Constraint: 1\n",
      "Training acc: 75.98%\n",
      "Testing acc: 71.86%\n",
      "537/537 [==============================] - 1s 1ms/step\n",
      "231/231 [==============================] - 0s 61us/step\n",
      "\n",
      "\n",
      "Drop rate: 0.6\n",
      "Weight Constraint: 2\n",
      "Training acc: 88.08%\n",
      "Testing acc: 83.12%\n",
      "537/537 [==============================] - 1s 1ms/step\n",
      "231/231 [==============================] - 0s 58us/step\n",
      "\n",
      "\n",
      "Drop rate: 0.6\n",
      "Weight Constraint: 3\n",
      "Training acc: 89.39%\n",
      "Testing acc: 84.85%\n",
      "537/537 [==============================] - 1s 1ms/step\n",
      "231/231 [==============================] - 0s 54us/step\n",
      "\n",
      "\n",
      "Drop rate: 0.6\n",
      "Weight Constraint: 4\n",
      "Training acc: 89.76%\n",
      "Testing acc: 84.85%\n",
      "537/537 [==============================] - 1s 1ms/step\n",
      "231/231 [==============================] - 0s 63us/step\n",
      "\n",
      "\n",
      "Drop rate: 0.6\n",
      "Weight Constraint: 5\n",
      "Training acc: 89.20%\n",
      "Testing acc: 86.15%\n",
      "537/537 [==============================] - 1s 1ms/step\n",
      "231/231 [==============================] - 0s 63us/step\n",
      "\n",
      "\n",
      "Drop rate: 0.7\n",
      "Weight Constraint: 1\n",
      "Training acc: 75.98%\n",
      "Testing acc: 71.86%\n",
      "537/537 [==============================] - 1s 1ms/step\n",
      "231/231 [==============================] - 0s 54us/step\n",
      "\n",
      "\n",
      "Drop rate: 0.7\n",
      "Weight Constraint: 2\n",
      "Training acc: 86.22%\n",
      "Testing acc: 81.82%\n",
      "537/537 [==============================] - 1s 1ms/step\n",
      "231/231 [==============================] - 0s 59us/step\n",
      "\n",
      "\n",
      "Drop rate: 0.7\n",
      "Weight Constraint: 3\n",
      "Training acc: 86.96%\n",
      "Testing acc: 82.68%\n",
      "537/537 [==============================] - 1s 2ms/step\n",
      "231/231 [==============================] - 0s 61us/step\n",
      "\n",
      "\n",
      "Drop rate: 0.7\n",
      "Weight Constraint: 4\n",
      "Training acc: 88.27%\n",
      "Testing acc: 84.42%\n",
      "537/537 [==============================] - 1s 1ms/step\n",
      "231/231 [==============================] - 0s 56us/step\n",
      "\n",
      "\n",
      "Drop rate: 0.7\n",
      "Weight Constraint: 5\n",
      "Training acc: 89.20%\n",
      "Testing acc: 84.42%\n",
      "537/537 [==============================] - 1s 2ms/step\n",
      "231/231 [==============================] - 0s 54us/step\n",
      "\n",
      "\n",
      "Drop rate: 0.8\n",
      "Weight Constraint: 1\n",
      "Training acc: 75.98%\n",
      "Testing acc: 71.43%\n",
      "537/537 [==============================] - 1s 2ms/step\n",
      "231/231 [==============================] - 0s 59us/step\n",
      "\n",
      "\n",
      "Drop rate: 0.8\n",
      "Weight Constraint: 2\n",
      "Training acc: 78.77%\n",
      "Testing acc: 74.03%\n",
      "537/537 [==============================] - 1s 2ms/step\n",
      "231/231 [==============================] - 0s 61us/step\n",
      "\n",
      "\n",
      "Drop rate: 0.8\n",
      "Weight Constraint: 3\n",
      "Training acc: 83.05%\n",
      "Testing acc: 80.09%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "537/537 [==============================] - 1s 2ms/step\n",
      "231/231 [==============================] - 0s 59us/step\n",
      "\n",
      "\n",
      "Drop rate: 0.8\n",
      "Weight Constraint: 4\n",
      "Training acc: 85.66%\n",
      "Testing acc: 82.25%\n",
      "537/537 [==============================] - 1s 2ms/step\n",
      "231/231 [==============================] - 0s 59us/step\n",
      "\n",
      "\n",
      "Drop rate: 0.8\n",
      "Weight Constraint: 5\n",
      "Training acc: 87.90%\n",
      "Testing acc: 83.12%\n",
      "537/537 [==============================] - 1s 2ms/step\n",
      "231/231 [==============================] - 0s 57us/step\n",
      "\n",
      "\n",
      "Drop rate: 0.9\n",
      "Weight Constraint: 1\n",
      "Training acc: 58.10%\n",
      "Testing acc: 51.95%\n",
      "537/537 [==============================] - 1s 2ms/step\n",
      "231/231 [==============================] - 0s 54us/step\n",
      "\n",
      "\n",
      "Drop rate: 0.9\n",
      "Weight Constraint: 2\n",
      "Training acc: 75.79%\n",
      "Testing acc: 71.43%\n",
      "537/537 [==============================] - 1s 2ms/step\n",
      "231/231 [==============================] - 0s 63us/step\n",
      "\n",
      "\n",
      "Drop rate: 0.9\n",
      "Weight Constraint: 3\n",
      "Training acc: 75.79%\n",
      "Testing acc: 71.43%\n",
      "537/537 [==============================] - 1s 2ms/step\n",
      "231/231 [==============================] - 0s 67us/step\n",
      "\n",
      "\n",
      "Drop rate: 0.9\n",
      "Weight Constraint: 4\n",
      "Training acc: 80.26%\n",
      "Testing acc: 75.32%\n",
      "537/537 [==============================] - 1s 2ms/step\n",
      "231/231 [==============================] - 0s 59us/step\n",
      "\n",
      "\n",
      "Drop rate: 0.9\n",
      "Weight Constraint: 5\n",
      "Training acc: 81.75%\n",
      "Testing acc: 79.22%\n"
     ]
    }
   ],
   "source": [
    "from keras.optimizers import Nadam\n",
    "from keras.constraints import maxnorm\n",
    "\n",
    "for drop in [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]:\n",
    "    for weight_constraint in [1,2,3,4,5]:\n",
    "    \n",
    "        np.random.seed(1)\n",
    "        \n",
    "        model = Sequential()\n",
    "        model.add(Dense(10, input_dim=16, kernel_initializer='he_normal',\n",
    "                        activation='softplus',kernel_constraint=maxnorm(weight_constraint)))\n",
    "        model.add(Dropout(drop))\n",
    "        model.add(Dense(3, activation='softmax'))\n",
    "        optim = Nadam(lr = 0.01)\n",
    "        model.compile(loss='categorical_crossentropy', optimizer=optim, metrics=['accuracy'])\n",
    "\n",
    "        seed = 1\n",
    "        np.random.seed(seed)\n",
    "        model.fit(X_train_scaled, y_train, epochs=200, batch_size=20, verbose=0)\n",
    "\n",
    "        scores_train = model.evaluate(X_train_scaled, y_train)\n",
    "        scores_test = model.evaluate(X_test_scaled, y_test)\n",
    "\n",
    "        print('\\n')\n",
    "        print('Drop rate: ',end=\"\")\n",
    "        print(drop)\n",
    "        print('Weight Constraint: ',end=\"\")\n",
    "        print(weight_constraint)\n",
    "        print('Training ', end=\"\")\n",
    "        print(\"%s: %.2f%%\" % (model.metrics_names[1], scores_train[1]*100))\n",
    "        print('Testing ', end=\"\")\n",
    "        print(\"%s: %.2f%%\" % (model.metrics_names[1], scores_test[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best Dropout rate is 0.3 and best weight constraint is 3. But, it is reducing both train and test accuracy. So, this will not be continued further <br>\n",
    "Next, Finding the best value for **number of neurons in the hidden layer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "537/537 [==============================] - 1s 2ms/step\n",
      "231/231 [==============================] - 0s 56us/step\n",
      "\n",
      "\n",
      "Number of neurons: 1\n",
      "Training acc: 91.62%\n",
      "Testing acc: 87.45%\n",
      "537/537 [==============================] - 1s 2ms/step\n",
      "231/231 [==============================] - 0s 59us/step\n",
      "\n",
      "\n",
      "Number of neurons: 5\n",
      "Training acc: 96.46%\n",
      "Testing acc: 92.21%\n",
      "537/537 [==============================] - 1s 2ms/step\n",
      "231/231 [==============================] - 0s 50us/step\n",
      "\n",
      "\n",
      "Number of neurons: 7\n",
      "Training acc: 97.77%\n",
      "Testing acc: 92.64%\n",
      "537/537 [==============================] - 1s 2ms/step\n",
      "231/231 [==============================] - 0s 59us/step\n",
      "\n",
      "\n",
      "Number of neurons: 8\n",
      "Training acc: 98.14%\n",
      "Testing acc: 93.07%\n",
      "537/537 [==============================] - 1s 2ms/step\n",
      "231/231 [==============================] - 0s 56us/step\n",
      "\n",
      "\n",
      "Number of neurons: 10\n",
      "Training acc: 96.65%\n",
      "Testing acc: 93.07%\n",
      "537/537 [==============================] - 1s 2ms/step\n",
      "231/231 [==============================] - 0s 52us/step\n",
      "\n",
      "\n",
      "Number of neurons: 12\n",
      "Training acc: 97.39%\n",
      "Testing acc: 92.21%\n"
     ]
    }
   ],
   "source": [
    "from keras.optimizers import Nadam\n",
    "\n",
    "for neurons in [1, 5, 7, 8, 10, 12]:\n",
    "    \n",
    "    seed = 1\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Dense(neurons, input_dim=16, kernel_initializer='he_normal', activation='softplus'))\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "    optim = Nadam(lr = 0.1)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optim, metrics=['accuracy'])\n",
    "\n",
    "    model.fit(X_train_scaled, y_train, epochs=200, batch_size=20, verbose=0)\n",
    "\n",
    "    scores_train = model.evaluate(X_train_scaled, y_train)\n",
    "    scores_test = model.evaluate(X_test_scaled, y_test)\n",
    "\n",
    "    print('\\n')\n",
    "    print('Number of neurons: ',end=\"\")\n",
    "    print(neurons)\n",
    "    print('Training ', end=\"\")\n",
    "    print(\"%s: %.2f%%\" % (model.metrics_names[1], scores_train[1]*100))\n",
    "    print('Testing ', end=\"\")\n",
    "    print(\"%s: %.2f%%\" % (model.metrics_names[1], scores_test[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We can see with **5 neurons** we can get the best train and test accuracy ~ 95%<br>\n",
    "Now let us use the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "537/537 [==============================] - 1s 2ms/step\n",
      "231/231 [==============================] - 0s 61us/step\n",
      "\n",
      "\n",
      "Number of neurons: 12\n",
      "Training acc: 96.46%\n",
      "Testing acc: 94.37%\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_151 (Dense)            (None, 5)                 85        \n",
      "_________________________________________________________________\n",
      "dense_152 (Dense)            (None, 3)                 18        \n",
      "=================================================================\n",
      "Total params: 103\n",
      "Trainable params: 103\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.optimizers import Nadam\n",
    "\n",
    "seed = 1\n",
    "np.random.seed(seed)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(5, input_dim=16, kernel_initializer='he_normal', activation='softplus'))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "optim = Nadam(lr = 0.1)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optim, metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train_scaled, y_train, epochs=200, batch_size=20, verbose=0)\n",
    "\n",
    "scores_train = model.evaluate(X_train_scaled, y_train)\n",
    "scores_test = model.evaluate(X_test_scaled, y_test)\n",
    "\n",
    "print('\\n')\n",
    "print('Number of neurons: ',end=\"\")\n",
    "print(neurons)\n",
    "print('Training ', end=\"\")\n",
    "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores_train[1]*100))\n",
    "print('Testing ', end=\"\")\n",
    "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores_test[1]*100))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The model summary is shown above and this will be used for classification of the overall load. <br>\n",
    "Overall we can see that in most of the models the training accuracy is higher than the test accuracy and even if we vary the parameters the train and test accuracy both are decreasing. This means we need **more data** for the variance problem."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
